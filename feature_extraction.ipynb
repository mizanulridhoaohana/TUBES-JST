{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing, Feature Extraction and Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage import io, color, img_as_ubyte\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/CORROSION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(class_output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Iterate through the images in the class folder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(class_dir):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# Adjust the file extension as needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/feature-extraction/feature_extraction.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         input_image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(class_dir, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/CORROSION'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "root_dir = './data'\n",
    "\n",
    "# Define the target size\n",
    "target_size = (256, 256)\n",
    "\n",
    "# Output directory for resized images\n",
    "output_dir = './resize_data'\n",
    "\n",
    "# Function to resize and add padding to an image\n",
    "def resize_and_add_padding(image_path, output_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = ImageOps.fit(image, target_size, method=0, bleed=0.0, centering=(0.5, 0.5))\n",
    "    image.save(output_path)\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through the \"CORROSION\" and \"NOCORROSION\" folders\n",
    "for class_folder in ['CORROSION', 'NOCORROSION']:\n",
    "    class_dir = os.path.join(root_dir, class_folder)\n",
    "\n",
    "    # Create a subdirectory in the output directory for each class\n",
    "    class_output_dir = os.path.join(output_dir, class_folder)\n",
    "    os.makedirs(class_output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the images in the class folder\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith('.jpg'):  # Adjust the file extension as needed\n",
    "            input_image_path = os.path.join(class_dir, filename)\n",
    "            output_image_path = os.path.join(class_output_dir, filename)\n",
    "            resize_and_add_padding(input_image_path, output_image_path)\n",
    "            print(f\"Resized: {input_image_path} -> {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gray Level Co-Occurance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_glcm_features(image_path):\n",
    "    img = io.imread(image_path)\n",
    "    gray = color.rgb2gray(img)\n",
    "    image = img_as_ubyte(gray)\n",
    "    \n",
    "    bins = np.array([0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 255])  # 16-bit\n",
    "    inds = np.digitize(image, bins)\n",
    "\n",
    "    max_value = inds.max() + 1\n",
    "    matrix_cooccurrence = graycomatrix(inds, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=max_value, normed=False, symmetric=False)\n",
    "\n",
    "    contrast = graycoprops(matrix_cooccurrence, 'contrast')\n",
    "    dissimilarity = graycoprops(matrix_cooccurrence, 'dissimilarity')\n",
    "    homogeneity = graycoprops(matrix_cooccurrence, 'homogeneity')\n",
    "    energy = graycoprops(matrix_cooccurrence, 'energy')\n",
    "    correlation = graycoprops(matrix_cooccurrence, 'correlation')\n",
    "    asm = graycoprops(matrix_cooccurrence, 'ASM')\n",
    "\n",
    "    return {\n",
    "        \"Contrast\": contrast,\n",
    "        \"Dissimilarity\": dissimilarity,\n",
    "        \"Homogeneity\": homogeneity,\n",
    "        \"Energy\": energy,\n",
    "        \"Correlation\": correlation,\n",
    "        \"ASM\": asm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrosion_dir = \"resize_data/CORROSION\"\n",
    "nocorrosion_dir = \"resize_data/NOCORROSION\"\n",
    "\n",
    "# Function to compute GLCM features for a directory of images\n",
    "def compute_glcm_features_for_directory(directory, label):\n",
    "    features_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            features = compute_glcm_features(image_path)\n",
    "            features[\"Label\"] = label  # Add the label\n",
    "            features_list.append(features)\n",
    "    return features_list\n",
    "\n",
    "# Compute GLCM features for both classes\n",
    "corrosion_features = compute_glcm_features_for_directory(corrosion_dir, \"corrosion\")\n",
    "nocorrosion_features = compute_glcm_features_for_directory(nocorrosion_dir, \"nocorrosion\")\n",
    "\n",
    "# Combine features for both classes\n",
    "all_features = corrosion_features + nocorrosion_features\n",
    "\n",
    "# Save as csv file\n",
    "feature_df = pd.DataFrame(all_features)\n",
    "feature_df.to_csv(\"./features/glcm_features.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the original DataFrame\n",
    "df_data = pd.read_csv('./features/glcm_features.csv')\n",
    "\n",
    "# Extract the 'Contrast' column and remove square brackets\n",
    "df_data['Contrast'] = df_data['Contrast'].str.replace('[', '').str.replace(']', '')\n",
    "df_data['Dissimilarity'] = df_data['Dissimilarity'].str.replace('[', '').str.replace(']', '')\n",
    "df_data['Homogeneity'] = df_data['Homogeneity'].str.replace('[', '').str.replace(']', '')\n",
    "df_data['Energy'] = df_data['Energy'].str.replace('[', '').str.replace(']', '')\n",
    "df_data['Correlation'] = df_data['Correlation'].str.replace('[', '').str.replace(']', '')\n",
    "df_data['ASM'] = df_data['ASM'].str.replace('[', '').str.replace(']', '')\n",
    "\n",
    "\n",
    "\n",
    "# Split the 'Contrast' column values by whitespace and expand them into separate columns\n",
    "df_data[['Contrast0', 'Contrast45', 'Contrast90', 'Contrast135']] = df_data['Contrast'].str.split(expand=True)\n",
    "df_data[['Dissimilarity0', 'Dissimilarity45', 'Dissimilarity90', 'Dissimilarity135']] = df_data['Dissimilarity'].str.split(expand=True)\n",
    "df_data[['Homogeneity0', 'Homogeneity45', 'Homogeneity90', 'Homogeneity135']] = df_data['Homogeneity'].str.split(expand=True)\n",
    "df_data[['Energy0', 'Energy45', 'Energy90', 'Energy135']] = df_data['Energy'].str.split(expand=True)\n",
    "df_data[['Correlation0', 'Correlation45', 'Correlation90', 'Correlation135']] = df_data['Correlation'].str.split(expand=True)\n",
    "df_data[['ASM0', 'ASM45', 'ASM90', 'ASM135']] = df_data['ASM'].str.split(expand=True)\n",
    "\n",
    "\n",
    "# Drop the original 'Contrast' column\n",
    "df_data.drop(columns=['Contrast'], inplace=True)\n",
    "df_data.drop(columns=['Dissimilarity'], inplace=True)\n",
    "df_data.drop(columns=['Homogeneity'], inplace=True)\n",
    "df_data.drop(columns=['Energy'], inplace=True)\n",
    "df_data.drop(columns=['Correlation'], inplace=True)\n",
    "df_data.drop(columns=['ASM'], inplace=True)\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a new CSV file\n",
    "new_csv_filename = './features/glcm_features_split.csv'\n",
    "df_data.to_csv(new_csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dengan PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   corrosion       0.73      0.81      0.77       192\n",
      " nocorrosion       0.75      0.64      0.69       166\n",
      "\n",
      "    accuracy                           0.73       358\n",
      "   macro avg       0.74      0.73      0.73       358\n",
      "weighted avg       0.74      0.73      0.73       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./features/glcm_features_split.csv')\n",
    "\n",
    "\n",
    "features = data.drop('Label', axis=1)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Encode labels (corrosion, no corrosion) to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = data['Label']\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_standardized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP classifier\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, solver='sgd', random_state=42, verbose=False)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6256983240223464, pca_component = 1\n",
      "Accuracy: 0.6256983240223464, pca_component = 2\n",
      "Accuracy: 0.6787709497206704, pca_component = 3\n",
      "Accuracy: 0.7374301675977654, pca_component = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7402234636871509, pca_component = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770949720670391, pca_component = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7513966480446927, pca_component = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7681564245810056, pca_component = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7737430167597765, pca_component = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7737430167597765, pca_component = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7681564245810056, pca_component = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.770949720670391, pca_component = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7821229050279329, pca_component = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7849162011173184, pca_component = 14\n",
      "Accuracy: 0.7988826815642458, pca_component = 15\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   corrosion       0.81      0.82      0.81       192\n",
      " nocorrosion       0.79      0.77      0.78       166\n",
      "\n",
      "    accuracy                           0.80       358\n",
      "   macro avg       0.80      0.80      0.80       358\n",
      "weighted avg       0.80      0.80      0.80       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset (assuming 'glcm_features_new.csv')\n",
    "data = pd.read_csv('./features/glcm_features_split.csv')\n",
    "\n",
    "features = data.drop('Label', axis=1)\n",
    "\n",
    "# Standardize the features (optional but recommended)\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "n_components = 15\n",
    "\n",
    "for i in range(n_components):\n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=i+1)\n",
    "    pca.fit(features_standardized)\n",
    "\n",
    "    # Transform the features\n",
    "    features_pca = pca.transform(features_standardized)\n",
    "\n",
    "    # Encode labels (corrosion, no corrosion) to numerical values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = data['Label']\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the MLP classifier\n",
    "    mlp_classifier = MLPClassifier(hidden_layer_sizes=(300, ), max_iter=200, random_state=42, verbose=False, solver='adam',alpha=0.0001,\n",
    "                                activation='relu', learning_rate='constant')\n",
    "    mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = mlp_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy}, pca_component = {i+1}\")\n",
    "print(f\"Classification Report:\\n{classification_rep}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/mlp_classifier_model.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained MLP classifier to a file\n",
    "model_filename = './model/mlp_classifier_model.joblib'\n",
    "joblib.dump(mlp_classifier, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=15 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Use PCA to reduce the dimensionality to 15\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m transformed_features_pca \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39;49mfit_transform(transformed_features_array)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Make predictions using the loaded model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m predictions \u001b[39m=\u001b[39m loaded_mlp_classifier\u001b[39m.\u001b[39mpredict(transformed_features_pca)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:460\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[1;32m    441\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m     U, S, Vt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[1;32m    461\u001b[0m     U \u001b[39m=\u001b[39m U[:, : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components_]\n\u001b[1;32m    463\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhiten:\n\u001b[1;32m    464\u001b[0m         \u001b[39m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:510\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[39m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_full(X, n_components)\n\u001b[1;32m    511\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39marpack\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrandomized\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_truncated(X, n_components, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_svd_solver)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/sklearn/decomposition/_pca.py:524\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    521\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmle\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is only supported if n_samples >= n_features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m         )\n\u001b[1;32m    523\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_components \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    525\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mn_components=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m must be between 0 and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    526\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmin(n_samples, n_features)=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    527\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msvd_solver=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (n_components, \u001b[39mmin\u001b[39m(n_samples, n_features))\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    530\u001b[0m \u001b[39m# Center data\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=15 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "model_filename = './model/mlp_classifier_model.joblib'\n",
    "loaded_mlp_classifier = joblib.load(model_filename)\n",
    "\n",
    "# Load the trained MLP classifier model from a filebackup FEATURE/feature_extraction.ipynb\n",
    "image_path = './resize_data/NOCORROSION/100e35cf19.jpg'\n",
    "target_size = (256, 256)\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize Image\n",
    "image_new = ImageOps.fit(image, target_size, method=0, bleed=0.0, centering=(0.5, 0.5))\n",
    "temp_image_path = './temp_resized_image.jpg'\n",
    "image_new.save(temp_image_path)\n",
    "\n",
    "# Extract GLCM Value\n",
    "features = compute_glcm_features(temp_image_path)\n",
    "\n",
    "# Initialize a new dictionary to store the transformed features\n",
    "transformed_features = {}\n",
    "\n",
    "# Define a list of angles\n",
    "angles = ['0', '45', '90', '135']\n",
    "\n",
    "# Iterate through the features and angles to create new labels\n",
    "for feature_name, feature_values in features.items():\n",
    "    for i, angle in enumerate(angles):\n",
    "        new_label = f'{feature_name}{angle}'\n",
    "        transformed_features[new_label] = feature_values[0][i]\n",
    "\n",
    "# Transform the dictionary to a 1D NumPy array\n",
    "transformed_features_array = np.array(list(transformed_features.values())).reshape(1, -1)\n",
    "\n",
    "# Use PCA to reduce the dimensionality to 15\n",
    "pca = PCA(n_components=15)\n",
    "transformed_features_pca = pca.fit_transform(transformed_features_array.reshape(1, -1))\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions = loaded_mlp_classifier.predict(transformed_features_pca)\n",
    "\n",
    "if predictions[0] == 0:\n",
    "    print(\"HASIL PREDIKSI: Corrosion\")\n",
    "else:\n",
    "    print(\"HASIL PREDIKSI: NoCorrosion\")\n",
    "\n",
    "# Close and remove the temporary image file\n",
    "image_new.close()\n",
    "if os.path.exists(temp_image_path):\n",
    "    os.remove(temp_image_path)\n",
    "\n",
    "image = plt.imread(image_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Binary Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data success!\n",
      "(1790, 256, 256)\n",
      "(1432, 256, 256)\n",
      "(358, 256, 256)\n",
      "(1432,)\n",
      "(358,)\n"
     ]
    }
   ],
   "source": [
    "def read_images_from_folder(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            label = subfolder  # Use the subfolder name as the label\n",
    "            for fn in os.listdir(subfolder_path):\n",
    "                if fn.endswith('.jpg'):\n",
    "                    img_path = os.path.join(subfolder_path, fn)\n",
    "                    im = Image.open(img_path).convert('L')\n",
    "                    data = np.array(im)\n",
    "                    images.append(data)\n",
    "                    labels.append(label)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Load images and labels from the 'resize_data' folder structure\n",
    "data_folder = './resize_data'\n",
    "images, labels = read_images_from_folder(data_folder)\n",
    "print('Load data success!')\n",
    "\n",
    "X = np.array(images)\n",
    "print(X.shape)\n",
    "\n",
    "# Encode labels (CORROSION, NOCORROSION) to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "radius = 2\n",
    "n_point = radius * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_texture(train_data, test_data):\n",
    "    max_bins_train = 0\n",
    "    max_bins_test = 0\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        lbp = feature.local_binary_pattern(train_data[i], n_point, radius, 'default')\n",
    "        max_bins_train = max(max_bins_train, int(lbp.max()) + 1)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        lbp = feature.local_binary_pattern(test_data[i], n_point, radius, 'default')\n",
    "        max_bins_test = max(max_bins_test, int(lbp.max()) + 1)\n",
    "\n",
    "    train_hist = np.zeros((len(train_data), max_bins_train))\n",
    "    test_hist = np.zeros((len(test_data), max_bins_test))\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "        lbp = feature.local_binary_pattern(train_data[i], n_point, radius, 'default')\n",
    "        train_hist[i], _ = np.histogram(lbp, bins=max_bins_train, density=True)\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        lbp = feature.local_binary_pattern(test_data[i], n_point, radius, 'default')\n",
    "        test_hist[i], _ = np.histogram(lbp, bins=max_bins_test, density=True)\n",
    "\n",
    "    return train_hist, test_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mizanul/Documents/code/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9636871508379888\n",
      "Test Accuracy: 0.8491620111731844\n",
      "Precision: 0.8502792586951002\n",
      "Recall: 0.8463227911646587\n",
      "F1 Score: 0.8476211495412556\n",
      "Overall Accuracy: 0.8491620111731844\n",
      "Overall Accuracy:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86       192\n",
      "           1       0.86      0.81      0.83       166\n",
      "\n",
      "    accuracy                           0.85       358\n",
      "   macro avg       0.85      0.85      0.85       358\n",
      "weighted avg       0.85      0.85      0.85       358\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from skimage import feature\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = lbp_texture(X_train, X_test)\n",
    "\n",
    "# Create and train an MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate the MLP classifier\n",
    "train_accuracy = mlp.score(X_train, y_train)\n",
    "test_accuracy = mlp.score(X_test, y_test)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "classify_report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"\\nOverall Accuracy: {classify_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model saved as ./model/mlp_lbp_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained MLP model to a file\n",
    "model_filename = './model/mlp_lbp_model.pkl'\n",
    "joblib.dump(mlp, model_filename)\n",
    "\n",
    "print(f\"MLP model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/mlp_lbp_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Call the LBP function on the single image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m lbp_features \u001b[39m=\u001b[39m lbp_texture(data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodel/mlp_lbp_model.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m prediction \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mpredict([lbp_features])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mizanul/Documents/code/TUBES-JST/feature_extraction.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction)\n",
      "File \u001b[0;32m~/Documents/code/.venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/mlp_lbp_model.pkl'"
     ]
    }
   ],
   "source": [
    "from skimage import feature\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def lbp_texture(image):\n",
    "    # Perform LBP feature extraction on a single image\n",
    "    lbp = feature.local_binary_pattern(image, n_point, radius, 'default')\n",
    "    max_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp, bins=max_bins, density=True)\n",
    "    return hist\n",
    "\n",
    "# Load and preprocess a single image\n",
    "image_path = './resize_data/NOCORROSION/01a5aee1ab.jpg'\n",
    "im = Image.open(image_path).convert('L')\n",
    "data = np.array(im)\n",
    "\n",
    "# Define LBP parameters\n",
    "radius = 2\n",
    "n_point = radius * 8\n",
    "\n",
    "# Call the LBP function on the single image\n",
    "lbp_features = lbp_texture(data)\n",
    "\n",
    "loaded_model = joblib.load('model/mlp_lbp_model.pkl')\n",
    "prediction = loaded_model.predict([lbp_features])\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
